/// <reference types="node" />
import { Agent, OutgoingHttpHeaders } from 'http';
import { UserOptions } from 'ibm-cloud-sdk-core';
import RecognizeStream = require('../lib/recognize-stream');
import GeneratedSpeechToTextV1 = require('./v1-generated');
declare class SpeechToTextV1 extends GeneratedSpeechToTextV1 {
    static ERR_NO_CORPORA: string;
    static ERR_TIMEOUT: string;
    constructor(options: UserOptions);
    /**
     * Waits while corpora analysis status is 'being_processes', fires callback once the status is 'analyzed'
     *
     * Note: the code will throw an error in case there in no corpus in the customization
     *
     *
     * @param {Object} params The parameters
     * @param {String} params.customization_id - The GUID of the custom language model
     * @param {Number} [params.interval=5000] - (milliseconds) - how long to wait between status checks
     * @param {Number} [params.times=30] - maximum number of attempts
     * @param {Function} callback
     */
    whenCorporaAnalyzed(params: SpeechToTextV1.WhenCorporaAnalyzedParams): Promise<any>;
    recognizeUsingWebSocket(params: SpeechToTextV1.RecognizeWebSocketParams): RecognizeStream;
    recognize(params: GeneratedSpeechToTextV1.RecognizeParams): Promise<GeneratedSpeechToTextV1.Response<GeneratedSpeechToTextV1.SpeechRecognitionResults>>;
    /**
     * Waits while a customization status is 'pending' or 'training', fires callback once the status is 'ready' or 'available'.
     *
     * Note: the customization will remain in 'pending' status until at least one word corpus is added.
     *
     * See http://www.ibm.com/watson/developercloud/speech-to-text/api/v1/#list_models for status details.
     *
     * @param {Object} params The parameters
     * @param {String} params.customization_id - The GUID of the custom language model
     * @param {Number} [params.interval=5000] - (milliseconds) - how log to wait between status checks
     * @param {Number} [params.times=30] - maximum number of attempts
     * @param {Function} callback
     */
    whenCustomizationReady(params: SpeechToTextV1.WhenCustomizationReadyParams): Promise<any>;
}
declare namespace SpeechToTextV1 {
    interface SpeechToTextError extends Error {
        message: string;
        code?: string;
    }
    interface CheckParams {
        /** How long to wait in milliseconds between status checks, defaults to 5000 milliseconds */
        interval: number;
        /** maximum number of attempts to check, defaults to 30 */
        times: number;
    }
    type WhenCorporaAnalyzedParams = GeneratedSpeechToTextV1.ListCorporaParams & CheckParams;
    interface WhenCorporaAnalyzedOptions extends WhenCorporaAnalyzedParams {
        errorFilter: (err: SpeechToTextError) => boolean;
    }
    type WhenCustomizationReadyParams = GeneratedSpeechToTextV1.GetLanguageModelParams & CheckParams;
    interface WhenCustomizationReadyOptions extends WhenCorporaAnalyzedParams {
        errorFilter: (err: SpeechToTextError) => boolean;
    }
    interface RecognizeWebSocketParams {
        headers?: OutgoingHttpHeaders;
        readableObjectMode?: boolean;
        objectMode?: boolean;
        agent?: Agent;
        accessToken?: string;
        watsonToken?: string;
        model?: string;
        languageCustomizationId?: string;
        acousticCustomizationId?: string;
        baseModelVersion?: string;
        xWatsonLearningOptOut?: boolean;
        xWatsonMetadata?: string;
        contentType?: string;
        customizationWeight?: number;
        inactivityTimeout?: number;
        interimResults?: boolean;
        keywords?: string[];
        keywordsThreshold?: number;
        maxAlternatives?: number;
        wordAlternativesThreshold?: number;
        wordConfidence?: boolean;
        timestamps?: boolean;
        profanityFilter?: boolean;
        smartFormatting?: boolean;
        speakerLabels?: boolean;
        grammarName?: string;
        redaction?: boolean;
        processingMetrics?: boolean;
        processingMetricsInterval?: number;
        audioMetrics?: boolean;
        endOfPhraseSilenceTime?: number;
        splitTranscriptAtPhraseEnd?: boolean;
        speechDetectorSensitivity?: number;
        backgroundAudioSuppression?: number;
    }
}
export = SpeechToTextV1;
